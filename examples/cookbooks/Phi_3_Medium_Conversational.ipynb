{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ddb8fd1c",
      "metadata": {
        "id": "ddb8fd1c"
      },
      "source": [
        "# Phi-3 Medium Conversational Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DhivyaBharathy-web/PraisonAI/blob/main/examples/cookbooks/Phi_3_Medium_Conversational.ipynb)\n"
      ],
      "metadata": {
        "id": "uMIvNFtYQDKO"
      },
      "id": "uMIvNFtYQDKO"
    },
    {
      "cell_type": "markdown",
      "id": "71a9292d",
      "metadata": {
        "id": "71a9292d"
      },
      "source": [
        "**Description:**\n",
        "\n",
        "Run a conversational inference using the Phi-3 Medium model with an efficient pipeline. The notebook illustrates basic loading, prompting, and response generation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9023e3f2",
      "metadata": {
        "id": "9023e3f2"
      },
      "source": [
        "**Dependencies**\n",
        "\n",
        "```python\n",
        "!pip install transformers accelerate\n",
        "!pip install torch torchvision\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ed8c345",
      "metadata": {
        "id": "8ed8c345"
      },
      "source": [
        "**Tools Used**\n",
        "\n",
        "- HuggingFace Transformers\n",
        "- Accelerate\n",
        "- PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68cb4bd6",
      "metadata": {
        "id": "68cb4bd6"
      },
      "source": [
        "**YAML Prompt**\n",
        "\n",
        "```yaml\n",
        "system: You are a helpful assistant.\n",
        "user: What is the capital of France?\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be5d0fde",
      "metadata": {
        "id": "be5d0fde"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-3-medium\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-3-medium\")\n",
        "\n",
        "prompt = \"What is the capital of France?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ac8c793",
      "metadata": {
        "id": "4ac8c793"
      },
      "source": [
        "**Output**\n",
        "\n",
        "This example shows the model answering a simple geography question.\n",
        "\n",
        "What is the capital of France? The capital of France is Paris.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}